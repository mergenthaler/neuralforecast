{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "from neuralforecast.models.components.transformer import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "from neuralforecast.models.components.selfattention import FullAttention, AttentionLayer\n",
    "from neuralforecast.models.components.embed import DataEmbedding\n",
    "from neuralforecast.losses.utils import LossFunction\n",
    "from neuralforecast.data.tsdataset import IterateWindowsDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Transformer with O(L^2) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, activation, e_layers,\n",
    "                 d_layers):\n",
    "        super(_Transformer, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                      output_attention=output_attention), d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomer model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, seq_len: int, \n",
    "                 label_len: int, pred_len: int, output_attention: bool,\n",
    "                 enc_in: int, dec_in: int, d_model: int, c_out: int, \n",
    "                 embed: str, freq: str, dropout: float, factor: float, \n",
    "                 n_heads: int, d_ff: int, activation: str, \n",
    "                 e_layers: int, d_layers: int,\n",
    "                 loss_train: str, loss_valid: str, loss_hypar: float, \n",
    "                 learning_rate: float, lr_decay: float, weight_decay: float, \n",
    "                 lr_decay_step_size: int, random_seed: int):\n",
    "        super(Transformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Vanilla Transformer model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len: int\n",
    "            Input sequence size.\n",
    "        label_len: int\n",
    "            Label sequence size.\n",
    "        pred_len: int\n",
    "            Prediction sequence size.\n",
    "        output_attention: bool\n",
    "            If true use output attention for Transformer model.\n",
    "        enc_in: int\n",
    "            Number of encoders in data embedding layers.\n",
    "        dec_in: int\n",
    "            Number of decoders in data embedding layers.\n",
    "        d_model: int\n",
    "            Number of nodes for embedding layers.\n",
    "        c_out: int\n",
    "            Number of output nodes in projection layer.\n",
    "        embed: str\n",
    "            Type of embedding layers.\n",
    "        freq: str\n",
    "            Frequency for embedding layers.\n",
    "        dropout: float\n",
    "            Float between (0, 1). Dropout for Transformer.\n",
    "        factor: float\n",
    "            Factor for attention layer.\n",
    "        n_heads: int\n",
    "            Number of heads in attention layer.\n",
    "        d_ff: int\n",
    "            Number of inputs in encoder layers.\n",
    "        activation: str\n",
    "            Activation function for encoder layer.\n",
    "        e_layers: int\n",
    "            Number of encoder layers.\n",
    "        d_layers: int\n",
    "            Number of decoder layers.\n",
    "        loss_train: str\n",
    "            Loss to optimize.\n",
    "            An item from ['MAPE', 'MASE', 'SMAPE', 'MSE', 'MAE', 'QUANTILE', 'QUANTILE2'].\n",
    "        loss_valid: str\n",
    "            Validation loss.\n",
    "            An item from ['MAPE', 'MASE', 'SMAPE', 'RMSE', 'MAE', 'QUANTILE'].\n",
    "        loss_hypar: float\n",
    "            Hyperparameter for chosen loss.\n",
    "        learning_rate: float\n",
    "            Learning rate between (0, 1).\n",
    "        lr_decay: float\n",
    "            Decreasing multiplier for the learning rate.\n",
    "        weight_decay: float\n",
    "            L2 penalty for optimizer.\n",
    "        lr_decay_step_size: int \n",
    "            Steps between each learning rate decay.\n",
    "        random_seed: int\n",
    "            random_seed for pseudo random pytorch initializer and\n",
    "            numpy random generator.\n",
    "        \"\"\"\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.seq_len = seq_len \n",
    "        self.label_len = label_len \n",
    "        self.pred_len = pred_len \n",
    "        self.output_attention = output_attention\n",
    "        self.enc_in = enc_in \n",
    "        self.dec_in = dec_in \n",
    "        self.d_model = d_model \n",
    "        self.c_out = c_out \n",
    "        self.embed = embed\n",
    "        self.freq = freq \n",
    "        self.dropout = dropout\n",
    "        self.factor = factor \n",
    "        self.n_heads = n_heads \n",
    "        self.d_ff = d_ff \n",
    "        self.activation = activation \n",
    "        self.e_layers = e_layers\n",
    "        self.d_layers = d_layers\n",
    "        \n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train, \n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters      \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.model = _Transformer(pred_len, output_attention,\n",
    "                                  enc_in, dec_in, d_model, c_out, \n",
    "                                  embed, freq, dropout,\n",
    "                                  factor, n_heads, d_ff, \n",
    "                                  activation, e_layers,\n",
    "                                  d_layers)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Autoformer needs batch of shape (batch_size, time, series) for y\n",
    "        and (batch_size, time, exogenous) for x\n",
    "        and doesnt need X for each time series.\n",
    "        USE DataLoader from pytorch instead of TimeSeriesLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        if batch['X'] is not None:\n",
    "            if batch['X'].dim()<4:\n",
    "                batch['X'] = batch['X'][None,:,:,:]\n",
    "        \n",
    "        if batch['sample_mask'].dim()<3:\n",
    "            batch['sample_mask'] = batch['sample_mask'][None,:,:]\n",
    "\n",
    "        Y = batch['Y'].permute(0, 2, 1)\n",
    "        X = batch['X'][:, 0, :, :].permute(0, 2, 1)\n",
    "        sample_mask = batch['sample_mask'].permute(0, 2, 1)\n",
    "        available_mask = batch['available_mask']\n",
    "        \n",
    "        s_begin = 0\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        batch_x = Y[:, s_begin:s_end, :]\n",
    "        batch_y = Y[:, r_begin:r_end, :]\n",
    "        batch_x_mark = X[:, s_begin:s_end, :]\n",
    "        batch_y_mark = X[:, r_begin:r_end, :]\n",
    "        outsample_mask = sample_mask[:, r_begin:r_end, :]\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :])\n",
    "        dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "        else:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "        batch_y = batch_y[:, -self.pred_len:, :]\n",
    "        outsample_mask = outsample_mask[:, -self.pred_len:, :]\n",
    "\n",
    "        return batch_y, forecast, outsample_mask, Y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        outsample_y, forecast, outsample_mask, Y = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        # Protection for missing batch_size dimension\n",
    "        if batch['Y'].dim()<3:\n",
    "            batch['Y'] = batch['Y'][None,:,:]\n",
    "\n",
    "        outsample_y, forecast, outsample_mask, Y = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: Transformer, Y_df: pd.DataFrame, X_df: pd.DataFrame = None, \n",
    "                S_df: pd.DataFrame = None, trainer: pl.Trainer =None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method for forecasting self.n_time_out periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    bath_size: int\n",
    "        Batch size for forecasting.\n",
    "    trainer: pl.Trainer\n",
    "        Trainer object for model training and evaluation.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    self.frequency = pd.infer_freq(Y_df[Y_df['unique_id']==Y_df['unique_id'][0]]['ds']) # Infer with first unique_id series\n",
    "\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.pred_len+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "    \n",
    "    # Dataset, loader and trainer\n",
    "    dataset = IterateWindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                    mask_df=None, f_cols=[],\n",
    "                                    input_size=self.seq_len,\n",
    "                                    output_size=self.pred_len,\n",
    "                                    ds_in_test=self.pred_len,\n",
    "                                    is_test=True,\n",
    "                                    verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if torch.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "\n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _, _ = [torch.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast = np.transpose(forecast, (0, 2, 1))\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.datasets.long_horizon import LongHorizon\n",
    "\n",
    "Y_df, X_df, S_df = LongHorizon.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df.reset_index(drop=True)\n",
    "Y_df.loc[Y_df['unique_id']=='OT','y'] = Y_df[Y_df['unique_id']=='OT']['y'] + 100 #To obseve differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc_model = {}\n",
    "\n",
    "mc_model['seq_len'] = 96\n",
    "mc_model['label_len'] = 48\n",
    "mc_model['pred_len'] = 96\n",
    "mc_model['output_attention'] = False\n",
    "mc_model['enc_in'] = 7\n",
    "mc_model['dec_in'] = 7\n",
    "mc_model['d_model'] = 512\n",
    "mc_model['c_out'] = 7\n",
    "mc_model['embed'] = 'timeF'\n",
    "mc_model['freq'] = 'h'\n",
    "mc_model['dropout'] = 0.05\n",
    "mc_model['factor'] = 1\n",
    "mc_model['n_heads'] = 8\n",
    "mc_model['d_ff'] = 2_048\n",
    "mc_model['activation'] = 'gelu'\n",
    "mc_model['e_layers'] = 2 \n",
    "mc_model['d_layers'] = 1\n",
    "mc_model['loss_train'] = 'MAE'\n",
    "mc_model['loss_hypar'] = 0.5\n",
    "mc_model['loss_valid'] = 'MAE'\n",
    "mc_model['learning_rate'] = 0.001\n",
    "mc_model['lr_decay'] = 0.5\n",
    "mc_model['weight_decay'] = 0.\n",
    "mc_model['lr_decay_step_size'] = 2\n",
    "mc_model['random_seed'] = 1\n",
    "\n",
    "# Dataset parameters\n",
    "mc_data = {}\n",
    "mc_data['mode'] = 'iterate_windows'\n",
    "mc_data['n_time_in'] = mc_model['seq_len']\n",
    "mc_data['n_time_out'] = mc_model['pred_len']\n",
    "mc_data['batch_size'] = 1\n",
    "mc_data['scaler'] = None\n",
    "mc_data['max_epochs'] = None\n",
    "mc_data['max_steps'] = 10\n",
    "mc_data['early_stop_patience'] = 20\n",
    "\n",
    "len_val = 11_520\n",
    "len_test = 11_520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.tsdataset import IterateWindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(mc=mc_data,\n",
    "                                                                     S_df=None, \n",
    "                                                                     Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=f_cols,\n",
    "                                                                     ds_in_val=len_val,\n",
    "                                                                     ds_in_test=len_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=int(mc_data['batch_size']),\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=int(mc_data['batch_size']),\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=int(mc_data['batch_size']),\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(**mc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            min_delta=1e-4, \n",
    "                                            patience=mc_data['early_stop_patience'],\n",
    "                                            verbose=False,\n",
    "                                            mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc_data['max_epochs'], \n",
    "                     max_steps=mc_data['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     num_sanity_val_steps=1,\n",
    "                     val_check_interval=1,\n",
    "                     limit_val_batches=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "#print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "#print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "#print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2017-10-24']\n",
    "Y_forecast_df = Y_forecast_df.reset_index(drop=True)\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forecast_df = X_df[X_df['ds']<'2017-10-25']\n",
    "X_forecast_df = X_forecast_df.reset_index(drop=True)\n",
    "X_forecast_df['ds'] = pd.to_datetime(X_forecast_df['ds'])\n",
    "X_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=X_forecast_df, S_df=S_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
