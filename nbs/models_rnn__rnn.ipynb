{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.rnn.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from hyperopt import hp\n",
    "\n",
    "from typing import Union, List\n",
    "from neuralforecast.models.components.drnn import DRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _RNN(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int,\n",
    "                 n_t: int, n_s: int, cell_type: str, dilations: list, state_hsize: int, add_nl_layer: bool):\n",
    "        super(_RNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_t = n_t\n",
    "        self.n_s = n_s\n",
    "        self.cell_type = cell_type\n",
    "        self.dilations = dilations\n",
    "        self.state_hsize = state_hsize\n",
    "        self.add_nl_layer = add_nl_layer\n",
    "        self.layers = len(dilations)\n",
    "\n",
    "        layers = []\n",
    "        for grp_num in range(len(self.dilations)):\n",
    "            if grp_num == 0:\n",
    "                input_size = self.input_size + (self.input_size + self.output_size)*self.n_t + self.n_s\n",
    "            else:\n",
    "                input_size = self.state_hsize\n",
    "            layer = DRNN(input_size,\n",
    "                         self.state_hsize,\n",
    "                         n_layers=len(self.dilations[grp_num]),\n",
    "                         dilations=self.dilations[grp_num],\n",
    "                         cell_type=self.cell_type)\n",
    "            layers.append(layer)\n",
    "\n",
    "        self.rnn_stack = nn.Sequential(*layers)\n",
    "\n",
    "        if self.add_nl_layer:\n",
    "            self.MLPW  = nn.Linear(self.state_hsize, self.state_hsize)\n",
    "\n",
    "        self.adapterW  = nn.Linear(self.state_hsize, self.output_size)\n",
    "\n",
    "    def forward(self, Y: t.Tensor, X: t.Tensor):\n",
    "        if self.n_t >0:\n",
    "            input_data = t.cat((Y, X), -1)\n",
    "        else:\n",
    "            input_data = Y\n",
    "        \n",
    "        for layer_num in range(len(self.rnn_stack)):\n",
    "            residual = input_data\n",
    "            output, _ = self.rnn_stack[layer_num](input_data)\n",
    "            if layer_num > 0:\n",
    "                output += residual\n",
    "            input_data = output\n",
    "\n",
    "        if self.add_nl_layer:\n",
    "            input_data = self.MLPW(input_data)\n",
    "            input_data = t.tanh(input_data)\n",
    "\n",
    "        input_data = self.adapterW(input_data)\n",
    "        input_data = input_data.transpose(0,1) #change to bs, n_windows\n",
    "        \n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Union, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from fastcore.foundation import patch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.losses.utils import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_size: int, output_size: int,\n",
    "                 n_x: int = 0, n_s: int = 0, \n",
    "                 sample_freq: int = 1,\n",
    "                 cell_type: str = 'LSTM', state_hsize: int = 50, \n",
    "                 dilations: List[List[int]] = [[1, 2], [4, 8]], \n",
    "                 add_nl_layer: bool = False,\n",
    "                 learning_rate: float = 1e-3, lr_scheduler_step_size: int = 1000,\n",
    "                 lr_decay: float = 0.9,\n",
    "                 gradient_eps: float = 1e-8, \n",
    "                 gradient_clipping_threshold: float = 20.,\n",
    "                 weight_decay: float = 0., noise_std: float = 1e-3,\n",
    "                 loss_train: str = 'MAE', loss_valid: str = 'MAE',\n",
    "                 loss_hypar: float = 0.,\n",
    "                 frequency: str = 'D',\n",
    "                 random_seed: int = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        \"\"\" Recurrent Neural Network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            input size of the recurrent neural network, usually a\n",
    "            multiple of seasonality\n",
    "        output_size: int\n",
    "            output_size or forecast horizon of the recurrent neural\n",
    "            network, usually multiple of seasonality\n",
    "        n_x: int\n",
    "            Number of temporal exogenous variables.\n",
    "        n_s: int\n",
    "            Number of static variables.\n",
    "        sample_freq: int\n",
    "            Step size between windows.\n",
    "        cell_type: str\n",
    "            Type of RNN cell, available GRU, LSTM, RNN, ResidualLSTM.\n",
    "        state_hsize: int\n",
    "            dimension of hidden state of the recurrent neural network\n",
    "        dilations: int list\n",
    "            each list represents one chunk of Dilated LSTMS, connected in\n",
    "            standard ResNet fashion\n",
    "        add_nl_layer: bool\n",
    "            whether to insert a tanh() layer between the RNN stack and the\n",
    "            linear adaptor (output) layers\n",
    "        learning_rate: float\n",
    "            size of the stochastic gradient descent steps\n",
    "        lr_scheduler_step_size: int\n",
    "            this step_size is the period for each learning rate decay\n",
    "        lr_decay: float\n",
    "            Learning rate decay.\n",
    "        gradient_eps: float\n",
    "            term added to the Adam optimizer denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        gradient_clipping_threshold: float\n",
    "            max norm of gradient vector, with all parameters treated\n",
    "            as a single vector\n",
    "        weight_decay: float\n",
    "            parameter to control classic L2/Tikhonov regularization\n",
    "            of the rnn parameters\n",
    "        noise_std: float\n",
    "            standard deviation of white noise added to input during\n",
    "            fit to avoid the model from memorizing the train data\n",
    "        loss_train: str\n",
    "            Loss used to train.\n",
    "        loss_valid: str\n",
    "            Loss used to validate.\n",
    "        loss_hypar: float\n",
    "            Hyperparameter for chosen loss.\n",
    "        frequency: str\n",
    "            Time series frequency.\n",
    "        random_seed: int\n",
    "            random_seed for pseudo random pytorch initializer and\n",
    "            numpy random generator.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.n_x = n_x\n",
    "        self.n_s = n_s \n",
    "        self.sample_freq = sample_freq\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_type = cell_type\n",
    "        self.state_hsize = state_hsize\n",
    "        self.dilations = dilations\n",
    "        self.add_nl_layer = add_nl_layer\n",
    "\n",
    "        # Regularization and optimization parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler_step_size = lr_scheduler_step_size\n",
    "        self.lr_decay = lr_decay\n",
    "        self.gradient_eps = gradient_eps\n",
    "        self.gradient_clipping_threshold = gradient_clipping_threshold\n",
    "        self.weight_decay = weight_decay\n",
    "        self.noise_std = noise_std\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train, \n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          percentile=self.loss_hypar,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "\n",
    "        self.frequency = frequency\n",
    "\n",
    "        #Defining model\n",
    "        self.model = _RNN(input_size=self.input_size,\n",
    "                          output_size=self.output_size,\n",
    "                          n_t=self.n_x, n_s=self.n_s,\n",
    "                          #noise_std=self.noise_std, \n",
    "                          cell_type=self.cell_type,\n",
    "                          dilations=self.dilations, \n",
    "                          state_hsize=self.state_hsize,\n",
    "                          add_nl_layer=self.add_nl_layer)\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "    def on_fit_start(self):\n",
    "        t.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def parse_batch(self, batch):\n",
    "        S = batch['S']\n",
    "        Y = batch['Y']\n",
    "        X = batch['X']\n",
    "        idxs = batch['ts_idxs']\n",
    "        sample_mask = batch['sample_mask']\n",
    "        available_mask = batch['available_mask']\n",
    "        \n",
    "        av_condition = t.nonzero(t.min(available_mask, axis=0).values)\n",
    "        min_time_stamp = int(av_condition.min())\n",
    "        sample_condition = t.nonzero(t.min(sample_mask, axis=0).values)\n",
    "        \n",
    "        if sample_condition.nelement() == 0:\n",
    "            max_time_stamp = int(av_condition.max())\n",
    "        else:\n",
    "            max_time_stamp = int(sample_condition.max())\n",
    "        available_ts = max_time_stamp - min_time_stamp + 1 # +1, inclusive counting\n",
    "        if available_ts < self.input_size + self.output_size:\n",
    "            raise Exception(\n",
    "                'Time series too short for given input and output size. \\n'\n",
    "                f'Available timestamps: {available_ts}'\n",
    "            )\n",
    "        \n",
    "        Y = Y[:, min_time_stamp:max_time_stamp + 1] #+1 because is not inclusive\n",
    "        X = X[:, :, min_time_stamp:max_time_stamp + 1]\n",
    "        sample_mask = sample_mask[:, min_time_stamp:max_time_stamp + 1]\n",
    "        available_mask = available_mask[:, min_time_stamp:max_time_stamp + 1]\n",
    "\n",
    "        Y = Y.unfold(dimension=-1, size=self.input_size+self.output_size, step=self.sample_freq)\n",
    "        X = X.unfold(dimension=-1, size=self.input_size+self.output_size, step=self.sample_freq)\n",
    "        sample_mask = sample_mask.unfold(dimension=-1, size=self.input_size+self.output_size, step=self.sample_freq)\n",
    "        available_mask = available_mask.unfold(dimension=-1, size=self.input_size+self.output_size, step=self.sample_freq)\n",
    "\n",
    "        Y = Y.transpose(0,1) # n_windows, batch, time\n",
    "        X = X.transpose(1,2) # batch, n_windows, n_s, time\n",
    "        X = X.flatten(start_dim=2)\n",
    "        X = X.transpose(0,1) # n_windows, batch, n_s*time\n",
    "        sample_mask = sample_mask.transpose(0,1)\n",
    "        available_mask = available_mask.transpose(0,1)\n",
    "        \n",
    "        return S, Y, X, idxs, sample_mask, available_mask\n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        rnn_opt = self.optimizers()\n",
    "        \n",
    "        # Parsing batch\n",
    "        S, Y, X, idxs, sample_mask, available_mask = self.parse_batch(batch)\n",
    "\n",
    "        insample_Y = Y[:,:, :self.input_size]\n",
    "        outsample_Y = Y[:,:, self.input_size:]\n",
    "        outsample_mask = sample_mask[:,:, self.input_size:]\n",
    "        outsample_Y = outsample_Y.transpose(0,1)\n",
    "        outsample_mask = outsample_mask.transpose(0,1)\n",
    "\n",
    "        y_hat = self.model(Y=insample_Y, X=X)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_Y,\n",
    "                                  y_hat=y_hat,\n",
    "                                  y_insample=Y, \n",
    "                                  mask=outsample_mask) \n",
    "        \n",
    "        rnn_opt.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                        max_norm=self.gradient_clipping_threshold)\n",
    "        rnn_opt.step()\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.lr_schedulers().step()\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        # Parsing batch\n",
    "        S, Y, X, idxs, sample_mask, available_mask = self.parse_batch(batch)\n",
    "        \n",
    "        insample_Y = Y[:,:, :self.input_size]\n",
    "        outsample_Y = Y[:,:, self.input_size:]\n",
    "        outsample_mask = sample_mask[:,:, self.input_size:]\n",
    "        outsample_Y = outsample_Y.transpose(0,1)\n",
    "        outsample_mask = outsample_mask.transpose(0,1)\n",
    "\n",
    "        y_hat = self.model(Y=insample_Y, X=X)\n",
    "        \n",
    "        loss = self.loss_fn_valid(y=outsample_Y,\n",
    "                                  y_hat=y_hat,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=Y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Parsing batch\n",
    "        S, Y, X, idxs, sample_mask, available_mask = self.parse_batch(batch)\n",
    "        ts_idxs = batch['ts_idxs']\n",
    "\n",
    "        insample_Y = Y[:,:, :self.input_size]\n",
    "        y_true = Y[:,:, self.input_size:]\n",
    "        sample_mask = sample_mask[:,:, self.input_size:]\n",
    "        y_true = y_true.transpose(0,1)\n",
    "        sample_mask = sample_mask.transpose(0,1)\n",
    "        \n",
    "        y_hat = self.model(Y=insample_Y, X=X)\n",
    "\n",
    "        # Filter to windows with at least one sampleable ts\n",
    "        y_true = y_true.reshape(-1, self.output_size)\n",
    "        y_hat = y_hat.reshape(-1, self.output_size)\n",
    "        sample_mask = sample_mask.reshape(-1, self.output_size)\n",
    "\n",
    "        sample_condition = (sample_mask.sum(dim=1) == self.output_size)\n",
    "\n",
    "        y_true=y_true[sample_condition,:]\n",
    "        y_hat=y_hat[sample_condition,:]\n",
    "        sample_mask=sample_mask[sample_condition,:]\n",
    "        \n",
    "        return y_true, y_hat, sample_mask, ts_idxs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        rnn_optimizer = Adam(params=self.model.parameters(),\n",
    "                             lr=self.learning_rate,\n",
    "                             eps=self.gradient_eps,\n",
    "                             weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_rnn = StepLR(optimizer=rnn_optimizer,\n",
    "                        step_size=self.lr_scheduler_step_size,\n",
    "                        gamma=self.lr_decay)\n",
    "        \n",
    "        return [rnn_optimizer], [lr_rnn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def forecast(self: RNN, Y_df, X_df = None, S_df = None, batch_size=1, trainer=None):\n",
    "    \"\"\"\n",
    "    Method for forecasting self.output_size periods after last timestamp of Y_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_df: pd.DataFrame\n",
    "        Dataframe with target time-series data, needs 'unique_id','ds' and 'y' columns.\n",
    "    X_df: pd.DataFrame\n",
    "        Dataframe with exogenous time-series data, needs 'unique_id' and 'ds' columns.\n",
    "        Note that 'unique_id' and 'ds' must match Y_df plus the forecasting horizon.\n",
    "    S_df: pd.DataFrame\n",
    "        Dataframe with static data, needs 'unique_id' column.\n",
    "    bath_size: int\n",
    "        Batch size for forecasting.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast_df: pd.DataFrame\n",
    "        Dataframe with forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add forecast dates to Y_df\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "    if X_df is not None:\n",
    "        X_df['ds'] = pd.to_datetime(X_df['ds'])\n",
    "    forecast_dates = pd.date_range(Y_df['ds'].max(), periods=self.output_size+1, freq=self.frequency)[1:]\n",
    "    index = pd.MultiIndex.from_product([Y_df['unique_id'].unique(), forecast_dates], names=['unique_id', 'ds'])\n",
    "    forecast_df = pd.DataFrame({'y':[0]}, index=index).reset_index()\n",
    "\n",
    "    Y_df = Y_df.append(forecast_df).sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    # Dataset, loader and trainer\n",
    "    dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df,\n",
    "                                S_df=S_df,\n",
    "                                ds_in_test=self.output_size,\n",
    "                                is_test=True,\n",
    "                                input_size=self.input_size,\n",
    "                                output_size=self.output_size,\n",
    "                                verbose=True)\n",
    "\n",
    "    loader = TimeSeriesLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "    if trainer is None:\n",
    "        gpus = -1 if t.cuda.is_available() else 0\n",
    "        trainer = pl.Trainer(progress_bar_refresh_rate=1,\n",
    "                             gpus=gpus,\n",
    "                             logger=False)\n",
    "\n",
    "    # Forecast\n",
    "    outputs = trainer.predict(self, loader)\n",
    "    \n",
    "    # Process forecast and include in forecast_df\n",
    "    _, forecast, _, _ = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "    forecast_df['y'] = forecast.flatten()\n",
    "\n",
    "    return forecast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.data.datasets.epf import EPF, EPFInfo\n",
    "\n",
    "import torch as t\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from neuralforecast.data.tsdataset import TimeSeriesDataset\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "from neuralforecast.losses.utils import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralforecast.data.datasets.epf import EPF\n",
    "from neuralforecast.data.tsloader import TimeSeriesLoader\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "FONTSIZE = 19\n",
    "\n",
    "# Load and plot data\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='./data', groups=['FR'])\n",
    "Y_df_2, X_df_2, S_df_2 = EPF.load_groups(directory='./data', groups=['NP'])\n",
    "Y_df_2['ds'] = Y_df['ds']\n",
    "X_df_2['ds'] = X_df['ds']\n",
    "Y_df = Y_df.append(Y_df_2).reset_index(drop=True)\n",
    "X_df = X_df.append(X_df_2).reset_index(drop=True)\n",
    "S_df = S_df.append(S_df_2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "\n",
    "# Trimming series to avoid slow backprop through time\n",
    "Y_df = Y_df.groupby('unique_id').tail(60*24+7*24).reset_index(drop=True)\n",
    "X_df = X_df.groupby('unique_id').tail(60*24+7*24).reset_index(drop=True)\n",
    "\n",
    "Y_df['y'] = Y_df['y']/Y_df['y'].max()\n",
    "\n",
    "train_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df,\n",
    "                                  ds_in_test=7*24,\n",
    "                                  is_test=False,\n",
    "                                  input_size=1*24,\n",
    "                                  output_size=24,\n",
    "                                  verbose=True)\n",
    "\n",
    "valid_dataset = TimeSeriesDataset(Y_df=Y_df, X_df=X_df,\n",
    "                                  ds_in_test=7*24,\n",
    "                                  is_test=True,\n",
    "                                  input_size=1*24,\n",
    "                                  output_size=24,\n",
    "                                  verbose=True)\n",
    "\n",
    "train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                batch_size=2,\n",
    "                                shuffle=True)\n",
    "\n",
    "valid_loader = TimeSeriesLoader(dataset=valid_dataset,\n",
    "                                batch_size=2,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(# Architecture parameters\n",
    "    n_s=train_dataset.n_s,\n",
    "    n_x=train_dataset.n_x,\n",
    "    input_size=3*train_dataset.input_size,\n",
    "    output_size=train_dataset.output_size,\n",
    "    sample_freq=train_dataset.output_size,\n",
    "    cell_type='LSTM',\n",
    "    state_hsize=50,\n",
    "    dilations=[[1, 2, 4, 8]],\n",
    "    add_nl_layer=False,\n",
    "    # Regularization and optimization parameters\n",
    "    learning_rate=1e-2,\n",
    "    lr_scheduler_step_size=333,\n",
    "    lr_decay=0.8,\n",
    "    gradient_eps=1e-8,\n",
    "    gradient_clipping_threshold=10,\n",
    "    weight_decay=0,\n",
    "    noise_std=0.0001,\n",
    "    loss_train='MAE',\n",
    "    loss_valid='MAE',\n",
    "    frequency='H',\n",
    "    random_seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                               min_delta=1e-4, \n",
    "                               patience=3, verbose=True, \n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, progress_bar_refresh_rate=1, \n",
    "                     log_every_n_steps=100, check_val_every_n_epoch=100,\n",
    "                     callbacks=[early_stopping])\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(model, valid_loader)\n",
    "\n",
    "y_true, y_hat, sample_mask, _ = zip(*outputs)\n",
    "y_true = t.cat(y_true).cpu()\n",
    "y_hat = t.cat(y_hat).cpu()\n",
    "sample_mask = t.cat(sample_mask).cpu()\n",
    "\n",
    "print(\"Original\")\n",
    "print(\"y_true.shape\", y_true.shape)\n",
    "print(\"y_hat.shape\", y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 7 * 24\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(y_true[0], color='#628793', linewidth=1, label='true')\n",
    "plt.plot(y_hat[0], color='peru', linewidth=1, label='forecast')\n",
    "plt.ylabel('Price [EUR/MWh]', fontsize=15)\n",
    "plt.xlabel('Date', fontsize=15)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_forecast_df = Y_df[Y_df['ds']<'2016-11-26'].reset_index(drop=True)\n",
    "Y_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forecast_df = X_df[X_df['ds']<'2016-11-27'].reset_index(drop=True)\n",
    "X_forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = model.forecast(Y_df=Y_forecast_df, X_df=X_forecast_df, S_df=S_df, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_df[Y_df['unique_id']=='FR']['y'][-24:].values)\n",
    "plt.plot(forecast_df['y'].values[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_df[Y_df['unique_id']=='NP']['y'][-24:].values)\n",
    "plt.plot(forecast_df['y'].values[24:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('neuralforecast')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
